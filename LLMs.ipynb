{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Less-Technical Definition\n",
    "\n",
    "A model designed to process and generate human-like text to perform tasks like answering questions, summarizing data, or translating languages. They’re “large” in that they were trained on a lot of data and have a range of capabilities. \n",
    "\n",
    "Examples: \n",
    "* GPT-3.5 and GPT-4 (OpenAI)\n",
    "* Claude and Claude 2 (Anthropic)\n",
    "* Bard (Google)\n",
    "* LLaMA and LLaMA 2 (Facebook/Open Source/many descendants)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A More Technical Definition\n",
    "\n",
    "A large language model (LLM) is a type of **deep learning neural network** (an algorithm inspired by the brain's structure, using layers of computational nodes to recognize patterns in data). Specifically, it's a variant of **transformer architectures** (a cutting-edge structure in machine learning introduced around 2017), which excels in processing language. \n",
    "\n",
    "The LLM operates by handling sequences of tokens (tokens can be words, parts of words, or characters, depending on how the text is split up) and its primary job is to predict subsequent **tokens** (basically guessing the next word in a sequence). \n",
    "\n",
    "These models are massive, having billions of **parameters** (adjustable parts of the model that help it learn from data). A key feature of transformers is their use of **attention mechanisms** (a technique allowing the model to \"focus\" on different parts of the input with varying intensities), which help weigh the importance of different input tokens when generating outputs.\n",
    "\n",
    "*This is the only part of the presentation I just took verbatim from GPT-4.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMs Are Text Synthesis Tools, And Coding Is Just Text\n",
    "\n",
    "* The text that LLMs were trained on includes a lot of code, so it can write code\n",
    "* It includes more in some languages than others, so it can code more in those languages\n",
    "* There are code-specific **models** and code-specific **benchmarks** - and you might choose to get deeper into either of those"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You Will Be Using These Tools In Your Coding On-The-Job\n",
    "\n",
    "[insert graph of stackoverflow traffic]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here A Few Ways How\n",
    "\n",
    "* Documentation (your readme file and your function-level documentation)\n",
    "* Writing code and making it better (especially if you're a beginner)\n",
    "* Unit tests\n",
    "* Giving personalized advice/troubleshooting on how to use tools, like git\n",
    "\n",
    "*But you still need to know how to do these things yourself, because it gets things wrong ALL THE TIME*\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
